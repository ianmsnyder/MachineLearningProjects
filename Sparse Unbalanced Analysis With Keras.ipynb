{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we will look at a sparse, unbalanced dataset, balance it with under-sampling and with SMOTE, and use deep neural networks with keras to attempt to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56020 entries, 0 to 56019\n",
      "Columns: 371 entries, ID to TARGET\n",
      "dtypes: float64(111), int64(260)\n",
      "memory usage: 158.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../train.csv\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    53814\n",
      "1     2206\n",
      "Name: TARGET, dtype: int64\n",
      "Ratio of fraud cases:\n",
      "0    0.960621\n",
      "1    0.039379\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fraudocc = df['TARGET'].value_counts()\n",
    "print(fraudocc)\n",
    "\n",
    "print(\"Ratio of fraud cases:\")\n",
    "print(fraudocc/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_360</th>\n",
       "      <th>var_361</th>\n",
       "      <th>var_362</th>\n",
       "      <th>var_363</th>\n",
       "      <th>var_364</th>\n",
       "      <th>var_365</th>\n",
       "      <th>var_366</th>\n",
       "      <th>var_367</th>\n",
       "      <th>var_368</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>56020.000000</td>\n",
       "      <td>5.602000e+04</td>\n",
       "      <td>56020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38006.087701</td>\n",
       "      <td>-1478.918761</td>\n",
       "      <td>33.176455</td>\n",
       "      <td>90.577048</td>\n",
       "      <td>71.914411</td>\n",
       "      <td>118.527881</td>\n",
       "      <td>3.308784</td>\n",
       "      <td>5.980358</td>\n",
       "      <td>0.351521</td>\n",
       "      <td>0.534811</td>\n",
       "      <td>...</td>\n",
       "      <td>7.061389</td>\n",
       "      <td>1.186156</td>\n",
       "      <td>12.682248</td>\n",
       "      <td>8.813963</td>\n",
       "      <td>33.439499</td>\n",
       "      <td>1.724232</td>\n",
       "      <td>76.339788</td>\n",
       "      <td>56.126032</td>\n",
       "      <td>1.170350e+05</td>\n",
       "      <td>0.039379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21965.342957</td>\n",
       "      <td>38463.619817</td>\n",
       "      <td>12.925293</td>\n",
       "      <td>1779.782881</td>\n",
       "      <td>338.520271</td>\n",
       "      <td>541.712773</td>\n",
       "      <td>89.095332</td>\n",
       "      <td>145.400358</td>\n",
       "      <td>19.645287</td>\n",
       "      <td>30.005879</td>\n",
       "      <td>...</td>\n",
       "      <td>427.464112</td>\n",
       "      <td>110.490491</td>\n",
       "      <td>852.152706</td>\n",
       "      <td>571.506477</td>\n",
       "      <td>2282.092562</td>\n",
       "      <td>146.961440</td>\n",
       "      <td>4244.942998</td>\n",
       "      <td>2960.886733</td>\n",
       "      <td>1.868549e+05</td>\n",
       "      <td>0.194496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.163750e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18941.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.792923e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38044.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.065634e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57075.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.188930e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>76019.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>12888.030000</td>\n",
       "      <td>21024.810000</td>\n",
       "      <td>8237.820000</td>\n",
       "      <td>11073.570000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>3810.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50003.880000</td>\n",
       "      <td>20385.720000</td>\n",
       "      <td>138831.630000</td>\n",
       "      <td>91778.730000</td>\n",
       "      <td>438329.220000</td>\n",
       "      <td>24650.010000</td>\n",
       "      <td>681462.900000</td>\n",
       "      <td>397884.300000</td>\n",
       "      <td>2.203474e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID          var_0         var_1          var_2         var_3  \\\n",
       "count  56020.000000   56020.000000  56020.000000   56020.000000  56020.000000   \n",
       "mean   38006.087701   -1478.918761     33.176455      90.577048     71.914411   \n",
       "std    21965.342957   38463.619817     12.925293    1779.782881    338.520271   \n",
       "min        0.000000 -999999.000000      5.000000       0.000000      0.000000   \n",
       "25%    18941.750000       2.000000     23.000000       0.000000      0.000000   \n",
       "50%    38044.500000       2.000000     28.000000       0.000000      0.000000   \n",
       "75%    57075.250000       2.000000     39.000000       0.000000      0.000000   \n",
       "max    76019.000000     238.000000    105.000000  210000.000000  12888.030000   \n",
       "\n",
       "              var_4         var_5         var_6         var_7         var_8  \\\n",
       "count  56020.000000  56020.000000  56020.000000  56020.000000  56020.000000   \n",
       "mean     118.527881      3.308784      5.980358      0.351521      0.534811   \n",
       "std      541.712773     89.095332    145.400358     19.645287     30.005879   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max    21024.810000   8237.820000  11073.570000   1800.000000   3810.000000   \n",
       "\n",
       "           ...            var_360       var_361        var_362       var_363  \\\n",
       "count      ...       56020.000000  56020.000000   56020.000000  56020.000000   \n",
       "mean       ...           7.061389      1.186156      12.682248      8.813963   \n",
       "std        ...         427.464112    110.490491     852.152706    571.506477   \n",
       "min        ...           0.000000      0.000000       0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000       0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000       0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000       0.000000      0.000000   \n",
       "max        ...       50003.880000  20385.720000  138831.630000  91778.730000   \n",
       "\n",
       "             var_364       var_365        var_366        var_367  \\\n",
       "count   56020.000000  56020.000000   56020.000000   56020.000000   \n",
       "mean       33.439499      1.724232      76.339788      56.126032   \n",
       "std      2282.092562    146.961440    4244.942998    2960.886733   \n",
       "min         0.000000      0.000000       0.000000       0.000000   \n",
       "25%         0.000000      0.000000       0.000000       0.000000   \n",
       "50%         0.000000      0.000000       0.000000       0.000000   \n",
       "75%         0.000000      0.000000       0.000000       0.000000   \n",
       "max    438329.220000  24650.010000  681462.900000  397884.300000   \n",
       "\n",
       "            var_368        TARGET  \n",
       "count  5.602000e+04  56020.000000  \n",
       "mean   1.170350e+05      0.039379  \n",
       "std    1.868549e+05      0.194496  \n",
       "min    5.163750e+03      0.000000  \n",
       "25%    6.792923e+04      0.000000  \n",
       "50%    1.065634e+05      0.000000  \n",
       "75%    1.188930e+05      0.000000  \n",
       "max    2.203474e+07      1.000000  \n",
       "\n",
       "[8 rows x 371 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It looks like -999999.000000 is the assignment for a NaN\n",
    "df=df.replace(-999999.000000,np.nan)\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_360</th>\n",
       "      <th>var_361</th>\n",
       "      <th>var_362</th>\n",
       "      <th>var_363</th>\n",
       "      <th>var_364</th>\n",
       "      <th>var_365</th>\n",
       "      <th>var_366</th>\n",
       "      <th>var_367</th>\n",
       "      <th>var_368</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40206</th>\n",
       "      <td>54652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117310.979016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  var_0  var_1  var_2  var_3  var_4  var_5  var_6  var_7  var_8  \\\n",
       "40206  54652    NaN     28    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "        ...    var_360  var_361  var_362  var_363  var_364  var_365  var_366  \\\n",
       "40206   ...        0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "       var_367        var_368  TARGET  \n",
       "40206      0.0  117310.979016       1  \n",
       "\n",
       "[1 rows x 371 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.isnull().any(axis=1)) & (df[\"TARGET\"]==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55937, 371)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are only 83 points and of them just 1 is a signal event, it should be safe to drop all the rows with NaN values\n",
    "df=df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.TARGET\n",
    "predictors=df.drop(columns=['ID','TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have to balance the data, since ML algorithms don't work well on very unbalanced datasets.  We will do this two ways: under-sample the majority class and synthesize new data points for the minority class and then compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ian/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0             1             2             3             4    \\\n",
      "count  55937.000000  55937.000000  55937.000000  55937.000000  55937.000000   \n",
      "mean       0.011334      0.281743      0.000432      0.005585      0.005643   \n",
      "std        0.039301      0.129270      0.008481      0.026278      0.025779   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.008403      0.180000      0.000000      0.000000      0.000000   \n",
      "50%        0.008403      0.230000      0.000000      0.000000      0.000000   \n",
      "75%        0.008403      0.340000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                5             6             7             8             9    \\\n",
      "count  55937.000000  55937.000000  55937.000000  55937.000000  55937.000000   \n",
      "mean       0.000402      0.000541      0.000196      0.000141      0.000358   \n",
      "std        0.010823      0.013140      0.010922      0.007881      0.010944   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "           ...                359           360           361           362  \\\n",
      "count      ...       55937.000000  55937.000000  55937.000000  55937.000000   \n",
      "mean       ...           0.000035      0.000141      0.000058      0.000091   \n",
      "std        ...           0.005873      0.008555      0.005424      0.006143   \n",
      "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                363           364           365           366           367  \\\n",
      "count  55937.000000  55937.000000  55937.000000  55937.000000  55937.000000   \n",
      "mean       0.000096      0.000076      0.000070      0.000112      0.000141   \n",
      "std        0.006232      0.005210      0.005966      0.006234      0.007447   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                368  \n",
      "count  55937.000000  \n",
      "mean       0.005078  \n",
      "std        0.008488  \n",
      "min        0.000000  \n",
      "25%        0.002848  \n",
      "50%        0.004598  \n",
      "75%        0.005166  \n",
      "max        1.000000  \n",
      "\n",
      "[8 rows x 369 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scaled=pd.DataFrame(sklearn.preprocessing.minmax_scale(predictors))\n",
    "print(pred_scaled.describe())\n",
    "pred_scaled.isnull().values.any() #check for any place we might have divided by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_undersampled, y_undersampled = rus.fit_resample(pred_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that undersampling worked:\n",
    "len([i for i in y_undersampled if i==1]) == len([i for i in y_undersampled if i==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = SMOTE(kind='regular')\n",
    "\n",
    "# Create the resampled feature set\n",
    "X_oversampled, y_oversampled = method.fit_sample(pred_scaled, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that oversampling worked:\n",
    "len([i for i in y_oversampled if i==1]) == len([i for i in y_oversampled if i==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing:  1_dense_layers_10_nodes_per_layer_1548668180.188044\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 10s 252us/step - loss: 0.1913 - acc: 0.9531 - val_loss: 0.1549 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 8s 212us/step - loss: 0.1599 - acc: 0.9613 - val_loss: 0.1513 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 8s 212us/step - loss: 0.1538 - acc: 0.9613 - val_loss: 0.1490 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 8s 210us/step - loss: 0.1482 - acc: 0.9613 - val_loss: 0.1463 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 9s 221us/step - loss: 0.1448 - acc: 0.9613 - val_loss: 0.1455 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 7s 187us/step - loss: 0.1421 - acc: 0.9613 - val_loss: 0.1472 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 6s 152us/step - loss: 0.1410 - acc: 0.9613 - val_loss: 0.1433 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 10s 266us/step - loss: 0.1393 - acc: 0.9613 - val_loss: 0.1454 - val_acc: 0.9589\n",
      "Epoch 9/30\n",
      "39155/39155 [==============================] - 11s 292us/step - loss: 0.1390 - acc: 0.9613 - val_loss: 0.1443 - val_acc: 0.9589\n",
      "Doing:  1_dense_layers_50_nodes_per_layer_1548668262.703777\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 11s 283us/step - loss: 0.1589 - acc: 0.9611 - val_loss: 0.1514 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 9s 240us/step - loss: 0.1476 - acc: 0.9613 - val_loss: 0.1473 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 9s 238us/step - loss: 0.1438 - acc: 0.9613 - val_loss: 0.1514 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 6s 154us/step - loss: 0.1417 - acc: 0.9613 - val_loss: 0.1441 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 8s 202us/step - loss: 0.1401 - acc: 0.9613 - val_loss: 0.1467 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 10s 254us/step - loss: 0.1393 - acc: 0.9613 - val_loss: 0.1442 - val_acc: 0.9589\n",
      "Doing:  1_dense_layers_100_nodes_per_layer_1548668324.5107598\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 14s 346us/step - loss: 0.1569 - acc: 0.9604 - val_loss: 0.1525 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 14s 370us/step - loss: 0.1469 - acc: 0.9613 - val_loss: 0.1471 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 15s 371us/step - loss: 0.1432 - acc: 0.9613 - val_loss: 0.1451 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 15s 375us/step - loss: 0.1411 - acc: 0.9613 - val_loss: 0.1463 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 13s 338us/step - loss: 0.1399 - acc: 0.9613 - val_loss: 0.1442 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 13s 341us/step - loss: 0.1389 - acc: 0.9613 - val_loss: 0.1429 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 14s 350us/step - loss: 0.1379 - acc: 0.9613 - val_loss: 0.1464 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 12s 312us/step - loss: 0.1383 - acc: 0.9612 - val_loss: 0.1441 - val_acc: 0.9589\n",
      "Doing:  1_dense_layers_150_nodes_per_layer_1548668439.4917812\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 17s 445us/step - loss: 0.1548 - acc: 0.9607 - val_loss: 0.1484 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 15s 372us/step - loss: 0.1459 - acc: 0.9613 - val_loss: 0.1469 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 14s 367us/step - loss: 0.1428 - acc: 0.9613 - val_loss: 0.1442 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 12s 306us/step - loss: 0.1412 - acc: 0.9613 - val_loss: 0.1435 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 9s 227us/step - loss: 0.1397 - acc: 0.9613 - val_loss: 0.1444 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 12s 299us/step - loss: 0.1387 - acc: 0.9613 - val_loss: 0.1435 - val_acc: 0.9589\n",
      "Doing:  2_dense_layers_10_nodes_per_layer_1548668528.239274\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 17s 436us/step - loss: 0.1839 - acc: 0.9593 - val_loss: 0.1534 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 14s 361us/step - loss: 0.1541 - acc: 0.9613 - val_loss: 0.1504 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 14s 368us/step - loss: 0.1498 - acc: 0.9613 - val_loss: 0.1471 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 14s 361us/step - loss: 0.1453 - acc: 0.9613 - val_loss: 0.1470 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 14s 350us/step - loss: 0.1454 - acc: 0.9613 - val_loss: 0.1462 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 10s 262us/step - loss: 0.1429 - acc: 0.9613 - val_loss: 0.1453 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 12s 317us/step - loss: 0.1421 - acc: 0.9613 - val_loss: 0.1451 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 13s 345us/step - loss: 0.1416 - acc: 0.9613 - val_loss: 0.1482 - val_acc: 0.9589\n",
      "Epoch 9/30\n",
      "39155/39155 [==============================] - 14s 350us/step - loss: 0.1403 - acc: 0.9613 - val_loss: 0.1449 - val_acc: 0.9589\n",
      "Epoch 10/30\n",
      "39155/39155 [==============================] - 13s 344us/step - loss: 0.1402 - acc: 0.9613 - val_loss: 0.1451 - val_acc: 0.9589\n",
      "Epoch 11/30\n",
      "39155/39155 [==============================] - 12s 306us/step - loss: 0.1397 - acc: 0.9613 - val_loss: 0.1457 - val_acc: 0.9589\n",
      "Doing:  2_dense_layers_50_nodes_per_layer_1548668687.841373\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 13s 331us/step - loss: 0.1628 - acc: 0.9604 - val_loss: 0.1508 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 11s 286us/step - loss: 0.1484 - acc: 0.9613 - val_loss: 0.1477 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 14s 362us/step - loss: 0.1441 - acc: 0.9613 - val_loss: 0.1461 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 15s 387us/step - loss: 0.1420 - acc: 0.9613 - val_loss: 0.1478 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 16s 401us/step - loss: 0.1402 - acc: 0.9613 - val_loss: 0.1468 - val_acc: 0.9589\n",
      "Doing:  2_dense_layers_100_nodes_per_layer_1548668764.6175628\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 17s 445us/step - loss: 0.1600 - acc: 0.9603 - val_loss: 0.1496 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 15s 387us/step - loss: 0.1466 - acc: 0.9613 - val_loss: 0.1481 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 13s 327us/step - loss: 0.1434 - acc: 0.9613 - val_loss: 0.1452 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 12s 307us/step - loss: 0.1413 - acc: 0.9613 - val_loss: 0.1437 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 15s 386us/step - loss: 0.1405 - acc: 0.9613 - val_loss: 0.1436 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 14s 348us/step - loss: 0.1394 - acc: 0.9613 - val_loss: 0.1449 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 13s 339us/step - loss: 0.1388 - acc: 0.9613 - val_loss: 0.1432 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 15s 373us/step - loss: 0.1385 - acc: 0.9613 - val_loss: 0.1445 - val_acc: 0.9589\n",
      "Epoch 9/30\n",
      "39155/39155 [==============================] - 15s 395us/step - loss: 0.1376 - acc: 0.9613 - val_loss: 0.1431 - val_acc: 0.9589\n",
      "Epoch 10/30\n",
      "39155/39155 [==============================] - 15s 388us/step - loss: 0.1369 - acc: 0.9613 - val_loss: 0.1456 - val_acc: 0.9589\n",
      "Epoch 11/30\n",
      "39155/39155 [==============================] - 12s 301us/step - loss: 0.1371 - acc: 0.9613 - val_loss: 0.1446 - val_acc: 0.9589\n",
      "Doing:  2_dense_layers_150_nodes_per_layer_1548668933.22709\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 19s 484us/step - loss: 0.1565 - acc: 0.9608 - val_loss: 0.1493 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 16s 420us/step - loss: 0.1468 - acc: 0.9613 - val_loss: 0.1456 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 16s 411us/step - loss: 0.1445 - acc: 0.9613 - val_loss: 0.1472 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 16s 407us/step - loss: 0.1414 - acc: 0.9613 - val_loss: 0.1434 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 16s 413us/step - loss: 0.1410 - acc: 0.9613 - val_loss: 0.1458 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 11s 293us/step - loss: 0.1394 - acc: 0.9613 - val_loss: 0.1531 - val_acc: 0.9589\n",
      "Doing:  3_dense_layers_10_nodes_per_layer_1548669040.818111\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 17s 424us/step - loss: 0.1914 - acc: 0.9606 - val_loss: 0.1574 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 16s 409us/step - loss: 0.1582 - acc: 0.9613 - val_loss: 0.1528 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 16s 403us/step - loss: 0.1521 - acc: 0.9613 - val_loss: 0.1496 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 16s 400us/step - loss: 0.1487 - acc: 0.9613 - val_loss: 0.1473 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 15s 395us/step - loss: 0.1462 - acc: 0.9613 - val_loss: 0.1486 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 11s 271us/step - loss: 0.1458 - acc: 0.9613 - val_loss: 0.1447 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 14s 349us/step - loss: 0.1435 - acc: 0.9613 - val_loss: 0.1447 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 15s 377us/step - loss: 0.1426 - acc: 0.9613 - val_loss: 0.1451 - val_acc: 0.9589\n",
      "Doing:  3_dense_layers_50_nodes_per_layer_1548669167.770041\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 17s 440us/step - loss: 0.1626 - acc: 0.9607 - val_loss: 0.1498 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 12s 318us/step - loss: 0.1485 - acc: 0.9613 - val_loss: 0.1461 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 9s 225us/step - loss: 0.1439 - acc: 0.9613 - val_loss: 0.1448 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 13s 325us/step - loss: 0.1426 - acc: 0.9613 - val_loss: 0.1443 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 12s 312us/step - loss: 0.1412 - acc: 0.9613 - val_loss: 0.1434 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 11s 290us/step - loss: 0.1402 - acc: 0.9613 - val_loss: 0.1448 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 11s 287us/step - loss: 0.1397 - acc: 0.9613 - val_loss: 0.1443 - val_acc: 0.9589\n",
      "Doing:  3_dense_layers_100_nodes_per_layer_1548669270.208011\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 17s 431us/step - loss: 0.1600 - acc: 0.9606 - val_loss: 0.1495 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 14s 359us/step - loss: 0.1472 - acc: 0.9613 - val_loss: 0.1479 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 15s 375us/step - loss: 0.1438 - acc: 0.9613 - val_loss: 0.1448 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 12s 316us/step - loss: 0.1419 - acc: 0.9613 - val_loss: 0.1482 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 13s 344us/step - loss: 0.1413 - acc: 0.9613 - val_loss: 0.1569 - val_acc: 0.9589\n",
      "Doing:  3_dense_layers_150_nodes_per_layer_1548669347.661082\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 19s 488us/step - loss: 0.1572 - acc: 0.9608 - val_loss: 0.1509 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 16s 416us/step - loss: 0.1477 - acc: 0.9613 - val_loss: 0.1461 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 16s 419us/step - loss: 0.1434 - acc: 0.9613 - val_loss: 0.1476 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 13s 324us/step - loss: 0.1419 - acc: 0.9613 - val_loss: 0.1476 - val_acc: 0.9589\n",
      "Doing:  4_dense_layers_10_nodes_per_layer_1548669432.835486\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 19s 489us/step - loss: 0.1840 - acc: 0.9613 - val_loss: 0.1557 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 15s 394us/step - loss: 0.1584 - acc: 0.9613 - val_loss: 0.1577 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 16s 402us/step - loss: 0.1529 - acc: 0.9613 - val_loss: 0.1539 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 14s 360us/step - loss: 0.1500 - acc: 0.9613 - val_loss: 0.1493 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 12s 299us/step - loss: 0.1491 - acc: 0.9613 - val_loss: 0.1491 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 14s 369us/step - loss: 0.1464 - acc: 0.9613 - val_loss: 0.1471 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 14s 356us/step - loss: 0.1451 - acc: 0.9613 - val_loss: 0.1477 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 14s 352us/step - loss: 0.1437 - acc: 0.9613 - val_loss: 0.1475 - val_acc: 0.9589\n",
      "Doing:  4_dense_layers_50_nodes_per_layer_1548669570.2601018\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 16s 403us/step - loss: 0.1654 - acc: 0.9603 - val_loss: 0.1615 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 16s 399us/step - loss: 0.1501 - acc: 0.9613 - val_loss: 0.1488 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 16s 417us/step - loss: 0.1460 - acc: 0.9613 - val_loss: 0.1472 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 16s 408us/step - loss: 0.1428 - acc: 0.9613 - val_loss: 0.1471 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 16s 397us/step - loss: 0.1427 - acc: 0.9613 - val_loss: 0.1441 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 16s 410us/step - loss: 0.1414 - acc: 0.9613 - val_loss: 0.1453 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 13s 323us/step - loss: 0.1403 - acc: 0.9613 - val_loss: 0.1495 - val_acc: 0.9589\n",
      "Doing:  4_dense_layers_100_nodes_per_layer_1548669700.250263\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 21s 549us/step - loss: 0.1608 - acc: 0.9613 - val_loss: 0.1509 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 18s 452us/step - loss: 0.1482 - acc: 0.9613 - val_loss: 0.1485 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 18s 447us/step - loss: 0.1438 - acc: 0.9613 - val_loss: 0.1464 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 14s 354us/step - loss: 0.1427 - acc: 0.9613 - val_loss: 0.1485 - val_acc: 0.9589\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39155/39155 [==============================] - 15s 389us/step - loss: 0.1418 - acc: 0.9613 - val_loss: 0.1515 - val_acc: 0.9589\n",
      "Doing:  4_dense_layers_150_nodes_per_layer_1548669806.577749\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 22s 571us/step - loss: 0.1586 - acc: 0.9611 - val_loss: 0.1547 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 18s 470us/step - loss: 0.1473 - acc: 0.9613 - val_loss: 0.1482 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 15s 378us/step - loss: 0.1443 - acc: 0.9613 - val_loss: 0.1529 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 19s 481us/step - loss: 0.1429 - acc: 0.9613 - val_loss: 0.1464 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 19s 487us/step - loss: 0.1416 - acc: 0.9613 - val_loss: 0.1477 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 19s 481us/step - loss: 0.1404 - acc: 0.9613 - val_loss: 0.1485 - val_acc: 0.9589\n",
      "Doing:  5_dense_layers_10_nodes_per_layer_1548669942.2419121\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 18s 468us/step - loss: 0.1996 - acc: 0.9605 - val_loss: 0.1590 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 18s 452us/step - loss: 0.1618 - acc: 0.9613 - val_loss: 0.1575 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 18s 451us/step - loss: 0.1573 - acc: 0.9613 - val_loss: 0.1556 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 17s 445us/step - loss: 0.1544 - acc: 0.9613 - val_loss: 0.1512 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 16s 416us/step - loss: 0.1502 - acc: 0.9613 - val_loss: 0.1487 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 12s 302us/step - loss: 0.1492 - acc: 0.9613 - val_loss: 0.1496 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 16s 416us/step - loss: 0.1481 - acc: 0.9613 - val_loss: 0.1493 - val_acc: 0.9589\n",
      "Doing:  5_dense_layers_50_nodes_per_layer_1548670083.291583\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 22s 570us/step - loss: 0.1673 - acc: 0.9600 - val_loss: 0.1562 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 16s 398us/step - loss: 0.1508 - acc: 0.9613 - val_loss: 0.1482 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 17s 443us/step - loss: 0.1465 - acc: 0.9613 - val_loss: 0.1461 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 19s 474us/step - loss: 0.1436 - acc: 0.9613 - val_loss: 0.1453 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 19s 478us/step - loss: 0.1421 - acc: 0.9613 - val_loss: 0.1448 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 18s 465us/step - loss: 0.1427 - acc: 0.9613 - val_loss: 0.1454 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 17s 430us/step - loss: 0.1419 - acc: 0.9613 - val_loss: 0.1463 - val_acc: 0.9589\n",
      "Doing:  5_dense_layers_100_nodes_per_layer_1548670238.65996\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 25s 626us/step - loss: 0.1621 - acc: 0.9611 - val_loss: 0.1503 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 19s 490us/step - loss: 0.1494 - acc: 0.9613 - val_loss: 0.1575 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 17s 436us/step - loss: 0.1449 - acc: 0.9613 - val_loss: 0.1469 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 17s 427us/step - loss: 0.1432 - acc: 0.9613 - val_loss: 0.1474 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 20s 518us/step - loss: 0.1419 - acc: 0.9613 - val_loss: 0.1490 - val_acc: 0.9589\n",
      "Doing:  5_dense_layers_150_nodes_per_layer_1548670362.876097\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 25s 642us/step - loss: 0.1626 - acc: 0.9610 - val_loss: 0.1522 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 19s 494us/step - loss: 0.1487 - acc: 0.9613 - val_loss: 0.1516 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 22s 563us/step - loss: 0.1458 - acc: 0.9613 - val_loss: 0.1441 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 22s 559us/step - loss: 0.1434 - acc: 0.9613 - val_loss: 0.1502 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 22s 558us/step - loss: 0.1427 - acc: 0.9613 - val_loss: 0.1457 - val_acc: 0.9589\n",
      "Doing:  6_dense_layers_10_nodes_per_layer_1548670502.9611928\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 24s 625us/step - loss: 0.1883 - acc: 0.9583 - val_loss: 0.1573 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 18s 456us/step - loss: 0.1616 - acc: 0.9613 - val_loss: 0.1563 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 17s 423us/step - loss: 0.1581 - acc: 0.9613 - val_loss: 0.1539 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 14s 360us/step - loss: 0.1551 - acc: 0.9613 - val_loss: 0.1523 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 17s 435us/step - loss: 0.1524 - acc: 0.9613 - val_loss: 0.1580 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 16s 409us/step - loss: 0.1485 - acc: 0.9613 - val_loss: 0.1542 - val_acc: 0.9589\n",
      "Doing:  6_dense_layers_50_nodes_per_layer_1548670639.412\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 25s 640us/step - loss: 0.1697 - acc: 0.9588 - val_loss: 0.1510 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 21s 540us/step - loss: 0.1526 - acc: 0.9613 - val_loss: 0.1506 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 18s 461us/step - loss: 0.1482 - acc: 0.9613 - val_loss: 0.1481 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 21s 534us/step - loss: 0.1457 - acc: 0.9613 - val_loss: 0.1545 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 21s 542us/step - loss: 0.1453 - acc: 0.9613 - val_loss: 0.1442 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 21s 545us/step - loss: 0.1435 - acc: 0.9613 - val_loss: 0.1456 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 21s 525us/step - loss: 0.1427 - acc: 0.9613 - val_loss: 0.1460 - val_acc: 0.9589\n",
      "Doing:  6_dense_layers_100_nodes_per_layer_1548670804.454765\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 28s 715us/step - loss: 0.1643 - acc: 0.9604 - val_loss: 0.1548 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 22s 571us/step - loss: 0.1505 - acc: 0.9613 - val_loss: 0.1498 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 17s 436us/step - loss: 0.1464 - acc: 0.9613 - val_loss: 0.1462 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 21s 549us/step - loss: 0.1440 - acc: 0.9613 - val_loss: 0.1453 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 23s 591us/step - loss: 0.1438 - acc: 0.9613 - val_loss: 0.1508 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 23s 586us/step - loss: 0.1431 - acc: 0.9613 - val_loss: 0.1449 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 19s 495us/step - loss: 0.1424 - acc: 0.9613 - val_loss: 0.1447 - val_acc: 0.9589\n",
      "Epoch 8/30\n",
      "39155/39155 [==============================] - 23s 589us/step - loss: 0.1404 - acc: 0.9613 - val_loss: 0.1440 - val_acc: 0.9589\n",
      "Epoch 9/30\n",
      "39155/39155 [==============================] - 23s 593us/step - loss: 0.1416 - acc: 0.9613 - val_loss: 0.1483 - val_acc: 0.9589\n",
      "Epoch 10/30\n",
      "39155/39155 [==============================] - 23s 584us/step - loss: 0.1412 - acc: 0.9613 - val_loss: 0.1443 - val_acc: 0.9589\n",
      "Doing:  6_dense_layers_150_nodes_per_layer_1548671062.790611\n",
      "Train on 39155 samples, validate on 16782 samples\n",
      "Epoch 1/30\n",
      "39155/39155 [==============================] - 31s 795us/step - loss: 0.1631 - acc: 0.9609 - val_loss: 0.1549 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "39155/39155 [==============================] - 26s 658us/step - loss: 0.1503 - acc: 0.9613 - val_loss: 0.1511 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "39155/39155 [==============================] - 20s 517us/step - loss: 0.1460 - acc: 0.9613 - val_loss: 0.1485 - val_acc: 0.9589\n",
      "Epoch 4/30\n",
      "39155/39155 [==============================] - 23s 585us/step - loss: 0.1490 - acc: 0.9613 - val_loss: 0.1455 - val_acc: 0.9589\n",
      "Epoch 5/30\n",
      "39155/39155 [==============================] - 25s 650us/step - loss: 0.1433 - acc: 0.9613 - val_loss: 0.1452 - val_acc: 0.9589\n",
      "Epoch 6/30\n",
      "39155/39155 [==============================] - 25s 640us/step - loss: 0.1430 - acc: 0.9613 - val_loss: 0.1456 - val_acc: 0.9589\n",
      "Epoch 7/30\n",
      "39155/39155 [==============================] - 22s 573us/step - loss: 0.1422 - acc: 0.9613 - val_loss: 0.1473 - val_acc: 0.9589\n"
     ]
    }
   ],
   "source": [
    "dense_layers=[1,2,3,4,5,6]\n",
    "n_nodes=[10, 50, 100, 150]\n",
    "n_cols = pred_scaled.shape[1]\n",
    "early_stopping_monitor = EarlyStopping(patience=2) #add a stopping factor so we don't have to run over all the epochs once they stop improving\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for n_node in n_nodes:\n",
    "        name=str(dense_layer)+\"_dense_layers_\"+str(n_node)+\"_nodes_per_layer_\"+str(time.time())\n",
    "        tensorboard = TensorBoard(log_dir='dnnlogs/'+str(name))\n",
    "        print(\"Doing: \", name)\n",
    "        \n",
    "        model=Sequential()\n",
    "        model.add(Dense(n_node, activation='relu', input_shape = (n_cols,)))\n",
    "        for l in range(dense_layer):\n",
    "            model.add(Dense(n_node, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "        \n",
    "\n",
    "        model.add(Dense(2, activation = 'softmax'))\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        model.fit(pred_scaled, target, validation_split=.3, epochs=30, callbacks=[tensorboard, early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an unbalanced dataset, of course this just assigned everything to be class 0 to get 96% accuracy.  Let's instead repeat on the over-sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing:  Oversampled_5_dense_layers_50_nodes_per_layer_1548725325.549073\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 23s 301us/step - loss: 0.4876 - acc: 0.7685 - val_loss: 0.9558 - val_acc: 0.5247\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 22s 286us/step - loss: 0.4554 - acc: 0.7908 - val_loss: 0.6783 - val_acc: 0.5735\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 20s 269us/step - loss: 0.4457 - acc: 0.7945 - val_loss: 0.8377 - val_acc: 0.5719\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 23s 306us/step - loss: 0.4405 - acc: 0.7972 - val_loss: 0.9356 - val_acc: 0.5235\n",
      "Doing:  Oversampled_5_dense_layers_100_nodes_per_layer_1548725429.128475\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 27s 358us/step - loss: 0.4806 - acc: 0.7744 - val_loss: 1.1206 - val_acc: 0.4456\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 26s 339us/step - loss: 0.4510 - acc: 0.7928 - val_loss: 0.9851 - val_acc: 0.4686\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 23s 308us/step - loss: 0.4401 - acc: 0.7978 - val_loss: 0.8943 - val_acc: 0.5761\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 24s 320us/step - loss: 0.4324 - acc: 0.8014 - val_loss: 0.8481 - val_acc: 0.6473\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 24s 318us/step - loss: 0.4267 - acc: 0.8028 - val_loss: 0.7120 - val_acc: 0.6212\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 23s 309us/step - loss: 0.4216 - acc: 0.8056 - val_loss: 0.7977 - val_acc: 0.5982\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 24s 312us/step - loss: 0.4173 - acc: 0.8067 - val_loss: 0.8419 - val_acc: 0.6364\n",
      "Doing:  Oversampled_5_dense_layers_200_nodes_per_layer_1548725615.149056\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 36s 475us/step - loss: 0.4808 - acc: 0.7747 - val_loss: 0.7692 - val_acc: 0.5975\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 33s 444us/step - loss: 0.4510 - acc: 0.7937 - val_loss: 0.8622 - val_acc: 0.5353\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 34s 453us/step - loss: 0.4401 - acc: 0.7974 - val_loss: 0.9534 - val_acc: 0.4731\n",
      "Doing:  Oversampled_5_dense_layers_300_nodes_per_layer_1548725734.517835\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 49s 657us/step - loss: 0.4806 - acc: 0.7764 - val_loss: 0.8364 - val_acc: 0.5524\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 47s 629us/step - loss: 0.4509 - acc: 0.7933 - val_loss: 0.8444 - val_acc: 0.5350\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 46s 611us/step - loss: 0.4397 - acc: 0.7988 - val_loss: 0.7315 - val_acc: 0.6293\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 48s 641us/step - loss: 0.4325 - acc: 0.8012 - val_loss: 0.9093 - val_acc: 0.5496\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 47s 623us/step - loss: 0.4243 - acc: 0.8037 - val_loss: 0.8845 - val_acc: 0.5500\n",
      "Doing:  Oversampled_8_dense_layers_50_nodes_per_layer_1548725988.836032\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 31s 415us/step - loss: 0.5015 - acc: 0.7555 - val_loss: 1.1275 - val_acc: 0.4072\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 24s 317us/step - loss: 0.4644 - acc: 0.7864 - val_loss: 0.6965 - val_acc: 0.5994\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 23s 305us/step - loss: 0.4547 - acc: 0.7912 - val_loss: 0.7180 - val_acc: 0.5574\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 24s 321us/step - loss: 0.4490 - acc: 0.7930 - val_loss: 0.7791 - val_acc: 0.5029\n",
      "Doing:  Oversampled_8_dense_layers_100_nodes_per_layer_1548726114.299048\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 28s 370us/step - loss: 0.4929 - acc: 0.7665 - val_loss: 0.8812 - val_acc: 0.5806\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 24s 325us/step - loss: 0.4605 - acc: 0.7867 - val_loss: 0.7789 - val_acc: 0.5392\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 25s 335us/step - loss: 0.4496 - acc: 0.7926 - val_loss: 0.7534 - val_acc: 0.5966\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 27s 360us/step - loss: 0.4413 - acc: 0.7974 - val_loss: 0.8368 - val_acc: 0.5640\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 27s 356us/step - loss: 0.4356 - acc: 0.7986 - val_loss: 0.6865 - val_acc: 0.6718\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 26s 352us/step - loss: 0.4327 - acc: 0.7996 - val_loss: 0.8731 - val_acc: 0.5770\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 26s 343us/step - loss: 0.4270 - acc: 0.8021 - val_loss: 0.7180 - val_acc: 0.5819\n",
      "Doing:  Oversampled_8_dense_layers_200_nodes_per_layer_1548726316.925301\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 39s 525us/step - loss: 0.4900 - acc: 0.7688 - val_loss: 0.8432 - val_acc: 0.5691\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 37s 489us/step - loss: 0.4580 - acc: 0.7901 - val_loss: 1.1157 - val_acc: 0.4336\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 41s 539us/step - loss: 0.4479 - acc: 0.7938 - val_loss: 0.9312 - val_acc: 0.5596\n",
      "Doing:  Oversampled_8_dense_layers_300_nodes_per_layer_1548726451.263793\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 59s 790us/step - loss: 0.4928 - acc: 0.7669 - val_loss: 0.6734 - val_acc: 0.6406\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 57s 762us/step - loss: 0.4608 - acc: 0.7867 - val_loss: 1.0108 - val_acc: 0.5686\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 58s 772us/step - loss: 0.4507 - acc: 0.7926 - val_loss: 0.7691 - val_acc: 0.5400\n",
      "Doing:  Oversampled_10_dense_layers_50_nodes_per_layer_1548726646.815518\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 29s 391us/step - loss: 0.5100 - acc: 0.7453 - val_loss: 0.7610 - val_acc: 0.4929\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 27s 360us/step - loss: 0.4707 - acc: 0.7823 - val_loss: 0.7645 - val_acc: 0.5476\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 28s 375us/step - loss: 0.4616 - acc: 0.7854 - val_loss: 0.7809 - val_acc: 0.5083\n",
      "Doing:  Oversampled_10_dense_layers_100_nodes_per_layer_1548726750.414623\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 32s 423us/step - loss: 0.5029 - acc: 0.7511 - val_loss: 0.9189 - val_acc: 0.4767\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 28s 369us/step - loss: 0.4669 - acc: 0.7827 - val_loss: 0.9224 - val_acc: 0.5175\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 27s 362us/step - loss: 0.4540 - acc: 0.7904 - val_loss: 0.9249 - val_acc: 0.5527\n",
      "Doing:  Oversampled_10_dense_layers_200_nodes_per_layer_1548726857.892247\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 42s 565us/step - loss: 0.5001 - acc: 0.7507 - val_loss: 0.8470 - val_acc: 0.5058\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 39s 523us/step - loss: 0.4614 - acc: 0.7881 - val_loss: 0.7531 - val_acc: 0.5867\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 39s 523us/step - loss: 0.4530 - acc: 0.7918 - val_loss: 0.8739 - val_acc: 0.5849\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 39s 524us/step - loss: 0.4483 - acc: 0.7931 - val_loss: 1.0297 - val_acc: 0.5180\n",
      "Doing:  Oversampled_10_dense_layers_300_nodes_per_layer_1548727037.4851189\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 67s 887us/step - loss: 0.5074 - acc: 0.7495 - val_loss: 0.8414 - val_acc: 0.5098\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 68s 905us/step - loss: 0.4710 - acc: 0.7798 - val_loss: 0.9205 - val_acc: 0.5197\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75224/75224 [==============================] - 66s 871us/step - loss: 0.4614 - acc: 0.7855 - val_loss: 0.9713 - val_acc: 0.1653\n",
      "Doing:  Oversampled_12_dense_layers_50_nodes_per_layer_1548727257.5796669\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 33s 440us/step - loss: 0.5114 - acc: 0.7440 - val_loss: 0.7968 - val_acc: 0.5122\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 31s 409us/step - loss: 0.4707 - acc: 0.7834 - val_loss: 0.6927 - val_acc: 0.6018\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 33s 442us/step - loss: 0.4592 - acc: 0.7891 - val_loss: 0.8328 - val_acc: 0.4907\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 34s 458us/step - loss: 0.4525 - acc: 0.7899 - val_loss: 0.6688 - val_acc: 0.5791\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 35s 471us/step - loss: 0.4509 - acc: 0.7878 - val_loss: 0.8331 - val_acc: 0.5118\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 34s 447us/step - loss: 0.4426 - acc: 0.7942 - val_loss: 0.7040 - val_acc: 0.6484\n",
      "Doing:  Oversampled_12_dense_layers_100_nodes_per_layer_1548727481.779949\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 40s 533us/step - loss: 0.5128 - acc: 0.7444 - val_loss: 0.8296 - val_acc: 0.5851\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 35s 460us/step - loss: 0.4727 - acc: 0.7803 - val_loss: 0.8511 - val_acc: 0.5172\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 33s 445us/step - loss: 0.4628 - acc: 0.7847 - val_loss: 0.8025 - val_acc: 0.6114\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 34s 451us/step - loss: 0.4541 - acc: 0.7897 - val_loss: 0.8051 - val_acc: 0.5839\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 35s 468us/step - loss: 0.4486 - acc: 0.7928 - val_loss: 1.0937 - val_acc: 0.4091\n",
      "Doing:  Oversampled_12_dense_layers_200_nodes_per_layer_1548727681.44167\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 55s 734us/step - loss: 0.5083 - acc: 0.7497 - val_loss: 1.1363 - val_acc: 0.4343\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 55s 736us/step - loss: 0.4760 - acc: 0.7694 - val_loss: 0.9524 - val_acc: 0.6439\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 58s 768us/step - loss: 0.4579 - acc: 0.7882 - val_loss: 0.9370 - val_acc: 0.5208\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 52s 695us/step - loss: 0.4515 - acc: 0.7919 - val_loss: 0.7014 - val_acc: 0.5943\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 57s 764us/step - loss: 0.4515 - acc: 0.7917 - val_loss: 0.7771 - val_acc: 0.5853\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 78s 1ms/step - loss: 0.4566 - acc: 0.7869 - val_loss: 0.8064 - val_acc: 0.6157\n",
      "Doing:  Oversampled_12_dense_layers_300_nodes_per_layer_1548728061.9122949\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 98s 1ms/step - loss: 0.5072 - acc: 0.7477 - val_loss: 1.2241 - val_acc: 0.5544\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 94s 1ms/step - loss: 0.4789 - acc: 0.7755 - val_loss: 0.9389 - val_acc: 0.5859\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 102s 1ms/step - loss: 0.4770 - acc: 0.7695 - val_loss: 1.0276 - val_acc: 0.1792\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 106s 1ms/step - loss: 0.4768 - acc: 0.7705 - val_loss: 0.9455 - val_acc: 0.5637\n"
     ]
    }
   ],
   "source": [
    "dense_layers=[5, 8, 10, 12]\n",
    "n_nodes=[50, 100, 200, 300]\n",
    "n_cols = pred_scaled.shape[1]\n",
    "early_stopping_monitor = EarlyStopping(patience=2) #add a stopping factor so we don't have to run over all the epochs once they stop improving\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for n_node in n_nodes:\n",
    "        name=\"Oversampled_\"+str(dense_layer)+\"_dense_layers_\"+str(n_node)+\"_nodes_per_layer_\"+str(time.time())\n",
    "        tensorboard = TensorBoard(log_dir='dnnlogs/'+str(name))\n",
    "        print(\"Doing: \", name)\n",
    "        \n",
    "        model=Sequential()\n",
    "        model.add(Dense(n_node, activation='relu', input_shape = (n_cols,)))\n",
    "        for l in range(dense_layer):\n",
    "            model.add(Dense(n_node, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "        \n",
    "\n",
    "        model.add(Dense(2, activation = 'softmax'))\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        model.fit(X_oversampled, y_oversampled, validation_split=.3, epochs=30, callbacks=[tensorboard, early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with 10 dense layers with 10 nodes per layer is the winner, so let's zero in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing:  Oversampled_9_dense_layers_5_nodes_per_layer_1548734278.737708\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 41s 549us/step - loss: 0.5834 - acc: 0.7143 - val_loss: 1.1374 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 32s 419us/step - loss: 0.5509 - acc: 0.7143 - val_loss: 0.9308 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 30s 403us/step - loss: 0.5389 - acc: 0.7143 - val_loss: 0.9576 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 30s 399us/step - loss: 0.5299 - acc: 0.7143 - val_loss: 0.9224 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 30s 404us/step - loss: 0.5241 - acc: 0.7143 - val_loss: 0.9710 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 30s 400us/step - loss: 0.5148 - acc: 0.7143 - val_loss: 0.8495 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 30s 398us/step - loss: 0.5133 - acc: 0.7143 - val_loss: 0.8374 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "75224/75224 [==============================] - 31s 409us/step - loss: 0.5100 - acc: 0.7143 - val_loss: 0.8112 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "75224/75224 [==============================] - 34s 452us/step - loss: 0.5087 - acc: 0.7143 - val_loss: 0.8971 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "75224/75224 [==============================] - 44s 588us/step - loss: 0.5049 - acc: 0.7143 - val_loss: 0.8618 - val_acc: 0.0000e+00\n",
      "Doing:  Oversampled_9_dense_layers_10_nodes_per_layer_1548734690.528951\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 47s 620us/step - loss: 0.5486 - acc: 0.7143 - val_loss: 0.8239 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 38s 510us/step - loss: 0.5079 - acc: 0.7213 - val_loss: 0.9014 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 36s 474us/step - loss: 0.4901 - acc: 0.7771 - val_loss: 0.7090 - val_acc: 0.7106\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 29s 389us/step - loss: 0.4788 - acc: 0.7837 - val_loss: 0.6787 - val_acc: 0.7502\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 30s 400us/step - loss: 0.4731 - acc: 0.7846 - val_loss: 0.6320 - val_acc: 0.8414\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 29s 388us/step - loss: 0.4683 - acc: 0.7883 - val_loss: 0.6478 - val_acc: 0.7447\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 29s 391us/step - loss: 0.4642 - acc: 0.7884 - val_loss: 0.6210 - val_acc: 0.8271\n",
      "Epoch 8/30\n",
      "75224/75224 [==============================] - 30s 398us/step - loss: 0.4623 - acc: 0.7900 - val_loss: 0.6713 - val_acc: 0.7131\n",
      "Epoch 9/30\n",
      "75224/75224 [==============================] - 30s 393us/step - loss: 0.4590 - acc: 0.7902 - val_loss: 0.6184 - val_acc: 0.8140\n",
      "Epoch 10/30\n",
      "75224/75224 [==============================] - 29s 391us/step - loss: 0.4582 - acc: 0.7905 - val_loss: 0.6593 - val_acc: 0.7357\n",
      "Epoch 11/30\n",
      "75224/75224 [==============================] - 35s 465us/step - loss: 0.4538 - acc: 0.7926 - val_loss: 0.6026 - val_acc: 0.8425\n",
      "Epoch 12/30\n",
      "75224/75224 [==============================] - 40s 534us/step - loss: 0.4511 - acc: 0.7918 - val_loss: 0.5613 - val_acc: 0.9266\n",
      "Epoch 13/30\n",
      "75224/75224 [==============================] - 38s 506us/step - loss: 0.4507 - acc: 0.7919 - val_loss: 0.6115 - val_acc: 0.8114\n",
      "Epoch 14/30\n",
      "75224/75224 [==============================] - 38s 502us/step - loss: 0.4500 - acc: 0.7917 - val_loss: 0.6438 - val_acc: 0.7667\n",
      "Doing:  Oversampled_9_dense_layers_15_nodes_per_layer_1548735217.859781\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 53s 701us/step - loss: 0.5335 - acc: 0.7169 - val_loss: 0.7616 - val_acc: 0.5633\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 44s 586us/step - loss: 0.4889 - acc: 0.7741 - val_loss: 0.6623 - val_acc: 0.7611\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 38s 509us/step - loss: 0.4760 - acc: 0.7817 - val_loss: 0.6908 - val_acc: 0.6447\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 41s 540us/step - loss: 0.4702 - acc: 0.7852 - val_loss: 0.6779 - val_acc: 0.6831\n",
      "Doing:  Oversampled_9_dense_layers_20_nodes_per_layer_1548735468.141168\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 45s 597us/step - loss: 0.5263 - acc: 0.7248 - val_loss: 0.9038 - val_acc: 0.5147\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 42s 559us/step - loss: 0.4843 - acc: 0.7616 - val_loss: 0.6709 - val_acc: 0.6987\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 42s 556us/step - loss: 0.4752 - acc: 0.7768 - val_loss: 0.7821 - val_acc: 0.5574\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 42s 559us/step - loss: 0.4651 - acc: 0.7848 - val_loss: 0.7152 - val_acc: 0.5319\n",
      "Doing:  Oversampled_10_dense_layers_5_nodes_per_layer_1548735676.4976902\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 46s 612us/step - loss: 0.5781 - acc: 0.7142 - val_loss: 1.0356 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 43s 569us/step - loss: 0.5528 - acc: 0.7143 - val_loss: 1.0493 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 39s 518us/step - loss: 0.5416 - acc: 0.7143 - val_loss: 0.9773 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 39s 515us/step - loss: 0.5346 - acc: 0.7143 - val_loss: 1.0000 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 41s 543us/step - loss: 0.5338 - acc: 0.7143 - val_loss: 0.9385 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 41s 548us/step - loss: 0.5307 - acc: 0.7143 - val_loss: 1.0194 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 36s 481us/step - loss: 0.5293 - acc: 0.7143 - val_loss: 1.0197 - val_acc: 0.0000e+00\n",
      "Doing:  Oversampled_10_dense_layers_10_nodes_per_layer_1548736000.384894\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 57s 751us/step - loss: 0.5514 - acc: 0.7141 - val_loss: 0.9664 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 50s 670us/step - loss: 0.5111 - acc: 0.7289 - val_loss: 0.8041 - val_acc: 0.6763\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 45s 593us/step - loss: 0.4924 - acc: 0.7570 - val_loss: 0.6981 - val_acc: 0.6932\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 46s 617us/step - loss: 0.4869 - acc: 0.7668 - val_loss: 0.8210 - val_acc: 0.5146\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 38s 508us/step - loss: 0.4796 - acc: 0.7686 - val_loss: 0.7068 - val_acc: 0.5252\n",
      "Doing:  Oversampled_10_dense_layers_15_nodes_per_layer_1548736312.880351\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 48s 632us/step - loss: 0.5410 - acc: 0.7142 - val_loss: 0.8424 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 40s 535us/step - loss: 0.5001 - acc: 0.7481 - val_loss: 0.7150 - val_acc: 0.5601\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 43s 573us/step - loss: 0.4848 - acc: 0.7744 - val_loss: 0.7130 - val_acc: 0.5615\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 37s 494us/step - loss: 0.4736 - acc: 0.7820 - val_loss: 0.6627 - val_acc: 0.6251\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 39s 514us/step - loss: 0.4696 - acc: 0.7861 - val_loss: 0.6173 - val_acc: 0.6886\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 38s 511us/step - loss: 0.4643 - acc: 0.7868 - val_loss: 0.6249 - val_acc: 0.6883\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 38s 505us/step - loss: 0.4594 - acc: 0.7883 - val_loss: 0.5742 - val_acc: 0.8458\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75224/75224 [==============================] - 40s 532us/step - loss: 0.4557 - acc: 0.7895 - val_loss: 0.6029 - val_acc: 0.7181\n",
      "Epoch 9/30\n",
      "75224/75224 [==============================] - 35s 460us/step - loss: 0.4540 - acc: 0.7914 - val_loss: 0.6028 - val_acc: 0.7350\n",
      "Doing:  Oversampled_10_dense_layers_20_nodes_per_layer_1548736754.44123\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 45s 594us/step - loss: 0.5307 - acc: 0.7203 - val_loss: 0.9456 - val_acc: 0.5343\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 37s 492us/step - loss: 0.4896 - acc: 0.7662 - val_loss: 0.8122 - val_acc: 0.5773\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 41s 543us/step - loss: 0.4726 - acc: 0.7832 - val_loss: 0.7170 - val_acc: 0.6998\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 54s 715us/step - loss: 0.4661 - acc: 0.7855 - val_loss: 0.7683 - val_acc: 0.6231\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 36s 478us/step - loss: 0.4621 - acc: 0.7876 - val_loss: 0.7146 - val_acc: 0.5909\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 45s 596us/step - loss: 0.4576 - acc: 0.7902 - val_loss: 0.6354 - val_acc: 0.6219\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 43s 578us/step - loss: 0.4502 - acc: 0.7927 - val_loss: 0.6303 - val_acc: 0.6959\n",
      "Epoch 8/30\n",
      "75224/75224 [==============================] - 44s 587us/step - loss: 0.4470 - acc: 0.7945 - val_loss: 0.6811 - val_acc: 0.5695\n",
      "Epoch 9/30\n",
      "75224/75224 [==============================] - 48s 640us/step - loss: 0.4442 - acc: 0.7957 - val_loss: 0.7442 - val_acc: 0.5401\n",
      "Doing:  Oversampled_11_dense_layers_5_nodes_per_layer_1548737196.9968781\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 46s 609us/step - loss: 0.5940 - acc: 0.7141 - val_loss: 1.1602 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 42s 564us/step - loss: 0.5677 - acc: 0.7143 - val_loss: 1.0963 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 57s 754us/step - loss: 0.5573 - acc: 0.7143 - val_loss: 1.0687 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 53s 708us/step - loss: 0.5486 - acc: 0.7143 - val_loss: 1.0290 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 55s 736us/step - loss: 0.5420 - acc: 0.7142 - val_loss: 0.9954 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 39s 516us/step - loss: 0.5312 - acc: 0.7142 - val_loss: 1.0098 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 42s 564us/step - loss: 0.5206 - acc: 0.7143 - val_loss: 0.9226 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "75224/75224 [==============================] - 49s 651us/step - loss: 0.5169 - acc: 0.7143 - val_loss: 0.9348 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "75224/75224 [==============================] - 47s 627us/step - loss: 0.5142 - acc: 0.7141 - val_loss: 0.9394 - val_acc: 0.0000e+00\n",
      "Doing:  Oversampled_11_dense_layers_10_nodes_per_layer_1548737682.680799\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 60s 792us/step - loss: 0.5530 - acc: 0.7142 - val_loss: 0.8710 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 42s 561us/step - loss: 0.5095 - acc: 0.7142 - val_loss: 0.8138 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 40s 525us/step - loss: 0.4957 - acc: 0.7217 - val_loss: 0.7480 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 34s 453us/step - loss: 0.4881 - acc: 0.7515 - val_loss: 0.7137 - val_acc: 0.7329\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 36s 476us/step - loss: 0.4808 - acc: 0.7692 - val_loss: 0.6782 - val_acc: 0.7668\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 35s 471us/step - loss: 0.4768 - acc: 0.7725 - val_loss: 0.7269 - val_acc: 0.6428\n",
      "Epoch 7/30\n",
      "75224/75224 [==============================] - 35s 468us/step - loss: 0.4717 - acc: 0.7747 - val_loss: 0.6519 - val_acc: 0.7992\n",
      "Epoch 8/30\n",
      "75224/75224 [==============================] - 37s 490us/step - loss: 0.4699 - acc: 0.7752 - val_loss: 0.6504 - val_acc: 0.7971\n",
      "Epoch 9/30\n",
      "75224/75224 [==============================] - 37s 488us/step - loss: 0.4672 - acc: 0.7762 - val_loss: 0.6248 - val_acc: 0.8912\n",
      "Epoch 10/30\n",
      "75224/75224 [==============================] - 37s 487us/step - loss: 0.4667 - acc: 0.7769 - val_loss: 0.6622 - val_acc: 0.7620\n",
      "Epoch 11/30\n",
      "75224/75224 [==============================] - 39s 513us/step - loss: 0.4630 - acc: 0.7790 - val_loss: 0.6277 - val_acc: 0.8741\n",
      "Doing:  Oversampled_11_dense_layers_15_nodes_per_layer_1548738161.445847\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 44s 587us/step - loss: 0.5444 - acc: 0.7143 - val_loss: 0.8713 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 35s 467us/step - loss: 0.4964 - acc: 0.7605 - val_loss: 0.7733 - val_acc: 0.4698\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 35s 464us/step - loss: 0.4821 - acc: 0.7695 - val_loss: 0.6915 - val_acc: 0.5213\n",
      "Epoch 4/30\n",
      "75224/75224 [==============================] - 35s 463us/step - loss: 0.4754 - acc: 0.7739 - val_loss: 0.5870 - val_acc: 0.7414\n",
      "Epoch 5/30\n",
      "75224/75224 [==============================] - 35s 461us/step - loss: 0.4708 - acc: 0.7772 - val_loss: 0.6726 - val_acc: 0.5540\n",
      "Epoch 6/30\n",
      "75224/75224 [==============================] - 35s 462us/step - loss: 0.4670 - acc: 0.7793 - val_loss: 0.6262 - val_acc: 0.6139\n",
      "Doing:  Oversampled_11_dense_layers_20_nodes_per_layer_1548738453.687288\n",
      "Train on 75224 samples, validate on 32240 samples\n",
      "Epoch 1/30\n",
      "75224/75224 [==============================] - 39s 520us/step - loss: 0.5443 - acc: 0.7160 - val_loss: 0.8820 - val_acc: 0.1797\n",
      "Epoch 2/30\n",
      "75224/75224 [==============================] - 36s 477us/step - loss: 0.4999 - acc: 0.7531 - val_loss: 0.9227 - val_acc: 0.4796\n",
      "Epoch 3/30\n",
      "75224/75224 [==============================] - 35s 468us/step - loss: 0.4807 - acc: 0.7739 - val_loss: 0.8825 - val_acc: 0.5721\n"
     ]
    }
   ],
   "source": [
    "dense_layers=[9, 10, 11]\n",
    "n_nodes=[5, 10, 15, 20]\n",
    "n_cols = pred_scaled.shape[1]\n",
    "early_stopping_monitor = EarlyStopping(patience=2) #add a stopping factor so we don't have to run over all the epochs once they stop improving\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for n_node in n_nodes:\n",
    "        name=\"Oversampled_\"+str(dense_layer)+\"_dense_layers_\"+str(n_node)+\"_nodes_per_layer_\"+str(time.time())\n",
    "        tensorboard = TensorBoard(log_dir='dnnlogs/'+str(name))\n",
    "        print(\"Doing: \", name)\n",
    "        \n",
    "        model=Sequential()\n",
    "        model.add(Dense(n_node, activation='relu', input_shape = (n_cols,)))\n",
    "        for l in range(dense_layer):\n",
    "            model.add(Dense(n_node, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "        \n",
    "\n",
    "        model.add(Dense(2, activation = 'softmax'))\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        model.fit(X_oversampled, y_oversampled, validation_split=.3, epochs=30, callbacks=[tensorboard, early_stopping_monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(x, y, model, oversample=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "    if oversample:\n",
    "        method = SMOTE(kind='regular')\n",
    "        X_train, y_train = method.fit_sample(X_train, y_train)\n",
    "    early_stopping_monitor = EarlyStopping(patience=2)\n",
    "    model.fit(X_train, y_train, validation_split=.3, epochs=30, callbacks=[early_stopping_monitor])\n",
    "    # Obtain model predictions\n",
    "    predicted = model.predict_classes(X_test)\n",
    "\n",
    "    # Print the classifcation report and confusion matrix\n",
    "    #print('Classification report:\\n', classification_report(y_test, predicted))\n",
    "    #conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "    #print('Confusion matrix:\\n', conf_mat)\n",
    "    \n",
    "    y_pred_k = model.predict_proba(X_test)#.ravel()\n",
    "    fpr=dict()\n",
    "    tpr=dict()\n",
    "    roc_auc=dict()\n",
    "    for i in range(2):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_k[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    #fpr, tpr, thresholds = roc_curve(ytest.ravel(), y_pred_k)\n",
    "    #print(set(y_pred_k))\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    #plt.plot(fpr[0], tpr[0], label='Class 0')#, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "    plt.plot(fpr[1], tpr[1], label='Class 1')# (area = {:.3f})'.format(auc_rf))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    #plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    print(\"Roc score: \", roc_auc_score(y_test, y_pred_k[:, 1]))\n",
    "    correct=0.\n",
    "    total=0.\n",
    "    print(\"Predicted: \", predicted[1])\n",
    "    print(\"y_test: \", type(y_test))\n",
    "    ytest=np.array(y_test)\n",
    "    print(ytest[0])\n",
    "    for i in range(len(ytest)):\n",
    "        total+=1.\n",
    "        if predicted[i]==ytest[i]: correct+=1.\n",
    "    print(\"Accuracy score: \", correct/total)#accuracy_score(predicted, y_test))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers=[10]\n",
    "n_nodes=[15]\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(n_node, activation='relu', input_shape = (n_cols,)))\n",
    "for l in range(dense_layer):\n",
    "    model.add(Dense(n_node, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "        \n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52642 samples, validate on 22562 samples\n",
      "Epoch 1/30\n",
      "52642/52642 [==============================] - 30s 564us/step - loss: 0.5140 - acc: 0.7402 - val_loss: 0.9246 - val_acc: 0.3738\n",
      "Epoch 2/30\n",
      "52642/52642 [==============================] - 32s 607us/step - loss: 0.5063 - acc: 0.7435 - val_loss: 1.0725 - val_acc: 0.3955\n",
      "Epoch 3/30\n",
      "52642/52642 [==============================] - 27s 508us/step - loss: 0.5409 - acc: 0.7243 - val_loss: 1.0148 - val_acc: 0.4699\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score:  0.7722056032466273\n",
      "Predicted:  0\n",
      "y_test:  <class 'pandas.core.series.Series'>\n",
      "0\n",
      "Accuracy score:  0.8677749970206173\n"
     ]
    }
   ],
   "source": [
    "get_model_results(predictors, target, model, oversample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
